---
title: "Practical Machine Learning, Writeup Course Project"
output: html_document
---

# Variables selection

From the list of all possible variables I assume that only variables with "accel" in their names represent data from accelerometers at different locations as it's required by the goal of the project. After quick analysis of the resulting data I also removed some variables which values are mostly NA (var_total_accel_belt, var_accel_arm, var_accel_dumbbell, var_accel_forearm) assuming that thier values are not significant for the final modeling (of course the classe variable is also presumed in the final data set). It's not clear whether some variables in the resulting data set are correlated with each other, so I use all remaining variables for the model fit.

```{r,eval=FALSE}
set.seed(2015)
data=read.csv("pml-training.csv")
accel_data=data[grepl("accel",names(data)) | grepl("classe", names(data))]
# Remove variables with many NA's
data_red = subset(accel_data,select=-c(var_total_accel_belt,var_accel_arm,var_accel_dumbbell,var_accel_forearm))
```

# Cross Validation
For the purpose of cross validation I split the data from the training.csv file into 2 subsets: actual training data (70% of the original data) and testing data (30%). Several different prediction algorithms are trained on the training set. After that the resulting models are used on the testing data set to check the accuracy of the prediction of that model. 

```{r,eval=FALSE}
inTrain = createDataPartition(y=data_red$classe,p=0.7,list=FALSE)
training = data_red[inTrain,]; testing = data_red[-inTrain,]
```

Because the amount of data in the original data set is pretty significant (13737 rows in the training data and 5885 rows in the testing data) comparing to the number of variables (only 17 are left after removing non-acceleration and NA variables) no other cross-validation methods (like K-fold or boostrap) are considered anymore.

# Model selection
Several different models are considered for prediction: linear model, tree, random forests, linear discriminant analysis and naive Bayes models. The table below specifies the accuracy of each model on the testing data set:

Model          |Accuracy (%)
---------------|------------
Linear         |52
Tree           |43
Random Forests |95
LDA            |51
Naive Bayes    |57

An example of the calculation of one model (linear model in this case):
```{r,eval=FALSE}
mf_lm=train(classe ~., methog='lm',data=training)
pred_lm=predict(mf_lm, testing)
confusionMatrix(pred_lm,testing$classe)
```
# Conclusion
From the table above I can see that the Random Forests prediction model has the best predicted accuracy on the testing data set. This model will be used to predict 20 different test cases from testing.csv file for the submisstion part of the project.
